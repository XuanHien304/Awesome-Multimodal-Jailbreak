## Jailbreak Defense of Multimodal LLM-based Models
|  Title  |   Venue  |   Date   |   Code   | Taxonomy |
|:--------|:--------:|:--------:|:--------:|:--------:|
|[**Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting**](https://arxiv.org/abs/2403.09513) | ECCV 2024 | 2024/05/14 | [Github](https://github.com/SaFoLab-WISC/AdaShield) | Input Level |
|[**Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts**](https://arxiv.org/abs/2311.09127) | Arxiv 2024 | 2024/01/20 | None | Input Level |
|[**Sim-clip: Unsupervised siamese adversarial fine-tuning for robust and semantically-rich vision-language models**](https://arxiv.org/abs/2407.14971) | Arxiv 2024 | 2024/07/20 | None | Encoder Level |
|[**Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks**](https://arxiv.org/abs/2409.07353) | Arxiv 2024 | 2024/09/11 | None | Encoder Level |
|[**Safety fine-tuning at (almost) no cost: A baseline for vision large language models**](https://arxiv.org/abs/2402.02207) | ICML 2024 | 2024/06/17 | [Github](https://github.com/ys-zong/VLGuard) | Model Level |
|[**Safety alignment for vision language models**](https://arxiv.org/abs/2405.13581) | Arxiv 2024 | 2024/05/22 | None | Model Level |
|[**Bathe: Defense against the jailbreak attack in multimodal large language models by treating harmful instruction as backdoor trigger**](https://arxiv.org/abs/2408.09093) | Arxiv 2024 | 2024/08/17 | None | Model Level |
|[**Cross-modal safety alignment: Is textual unlearning all you need?**](https://arxiv.org/abs/2406.02575) | Arxiv 2024 | 2024/05/27 | None | Model Level |
|[**Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions**](https://arxiv.org/abs/2309.07875) | ICLR 2024 | 2024/05/19 | [Github](https://github.com/vinid/safety-tuned-llamas) | Model Level |
|[**Mllm-protector: Ensuring mllmâ€™s safety without hurting performance**](https://arxiv.org/abs/2401.02906) | Arxiv 2024 | 2024/06/17 | [Github](https://github.com/pipilurj/MLLM-protector) | Output Level |
|[**Information-theoretical principled trade-off between jailbreakability and stealthiness on vision language models**](https://arxiv.org/abs/2410.01438) | Arxiv 2024 | 2024/10/02 | None | Input Level |
|[**Defending jailbreak attack in vlms via cross-modality information detector**](https://arxiv.org/abs/2407.21659) | Arxiv 2024 | 2024/10/11 | [Github](https://github.com/pandragonxiii/cider) | Encoder Level |
|[**Inferaligner: Inference-time alignment for harmlessness through cross-model guidance**](https://arxiv.org/abs/2401.11206) | Arxiv 2024 | 2024/01/20 | [Github](https://github.com/Jihuai-wpy/InferAligner) | Encoder Level |
|[**Jailguard: A universal detection framework for llm prompt-based attacks**](https://arxiv.org/abs/2312.10766) | Arxiv 2024 | 2024/06/18 | None | Encoder Level |
